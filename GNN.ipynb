{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import  \n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from geopy.distance import geodesic\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Step 1: Load and preprocess data\n",
    "# ----------------------------\n",
    "# Load the bike trip data\n",
    "data = pd.read_csv('zhong.csv')\n",
    "\n",
    "# Convert time columns to datetime format and extract features\n",
    "data['start_time'] = pd.to_datetime(data['start_time'])\n",
    "data['hour'] = data['start_time'].dt.hour\n",
    "\n",
    "# Calculate total demand for each station and hour\n",
    "demand_data = data.groupby(['end_station_name', 'hour']).size().rename('total_demand').reset_index()\n",
    "\n",
    "# Normalize total demand\n",
    "scaler = MinMaxScaler()\n",
    "demand_data['total_demand'] = scaler.fit_transform(demand_data[['total_demand']])\n",
    "\n",
    "# Add sine and cosine time features for periodicity\n",
    "demand_data['hour_sin'] = np.sin(2 * np.pi * demand_data['hour'] / 24)\n",
    "demand_data['hour_cos'] = np.cos(2 * np.pi * demand_data['hour'] / 24)\n",
    "\n",
    "# Assign station IDs\n",
    "demand_data['station_id'] = demand_data['end_station_name'].factorize()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Step 2: Create graph structure\n",
    "# ----------------------------\n",
    "# Load station coordinates for edges\n",
    "station_coords = data[['end_station_name', 'end_lat', 'end_lng']].drop_duplicates()\n",
    "station_coords = station_coords.dropna(subset=['end_lat', 'end_lng'])\n",
    "station_coords = station_coords[\n",
    "    (station_coords['end_lat'].apply(np.isfinite)) & (station_coords['end_lng'].apply(np.isfinite))\n",
    "]\n",
    "station_coords.columns = ['station_name', 'lat', 'lng']\n",
    "\n",
    "# Build edge connections based on station proximity\n",
    "edges = []\n",
    "edge_weights = []\n",
    "stations = station_coords.to_dict('records')\n",
    "\n",
    "for i, station1 in enumerate(stations):\n",
    "    for j, station2 in enumerate(stations):\n",
    "        if i != j:\n",
    "            distance = geodesic((station1['lat'], station1['lng']), (station2['lat'], station2['lng'])).kilometers\n",
    "            if distance <= 1.0:  # Only consider stations within 1 km\n",
    "                edges.append((station1['station_name'], station2['station_name']))\n",
    "                edge_weights.append(1 / distance)  # Weight inversely proportional to distance\n",
    "\n",
    "# Convert station names to indices for PyTorch Geometric\n",
    "station_index_map = dict(zip(demand_data['end_station_name'], demand_data['station_id']))\n",
    "edges = [(station_index_map[edge[0]], station_index_map[edge[1]]) for edge in edges]\n",
    "edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "edge_attr = torch.tensor(edge_weights, dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Step 3: Prepare node features and labels\n",
    "# ----------------------------\n",
    "\n",
    "# Get the number of stations and the embedding dimension\n",
    "num_stations = demand_data['station_id'].nunique()\n",
    "station_embedding_dim = 8  # Embedding dimension\n",
    "\n",
    "# Convert station IDs to tensors\n",
    "station_ids = torch.tensor(demand_data['station_id'].values, dtype=torch.long)  # [num_samples]\n",
    "\n",
    "# Convert time features to tensors\n",
    "time_features = torch.tensor(demand_data[['hour_sin', 'hour_cos']].values, dtype=torch.float)  # [num_samples, 2]\n",
    "\n",
    "# Prepare labels\n",
    "labels = demand_data['total_demand'].values\n",
    "y = torch.tensor(labels, dtype=torch.float).unsqueeze(1)  # [num_samples, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Step 4: Split data into train/test\n",
    "# ----------------------------\n",
    "train_mask, test_mask = train_test_split(range(x.size(0)), test_size=0.2, random_state=42)\n",
    "train_mask = torch.tensor(train_mask, dtype=torch.long)\n",
    "test_mask = torch.tensor(test_mask, dtype=torch.long)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Step 5: Define GNN model\n",
    "# ----------------------------\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_stations, station_embedding_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        # Define the embedding layer\n",
    "        self.station_embedding = nn.Embedding(num_stations, station_embedding_dim)  # Embedding layer\n",
    "        # Define graph convolutional layers\n",
    "        self.conv1 = GCNConv(input_dim + station_embedding_dim, hidden_dim)  # Input includes embedding dimension\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, time_features, station_ids, edge_index):\n",
    "        # Obtain station embeddings\n",
    "        station_embedded = self.station_embedding(station_ids)  # [num_samples, station_embedding_dim]\n",
    "        # Combine time features and station embeddings\n",
    "        x = torch.cat([time_features, station_embedded], dim=1)  # [num_samples, input_dim + station_embedding_dim]\n",
    "        # Apply graph convolution operations\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "# Get input dimension\n",
    "input_dim = time_features.size(1)  # Time feature dimensions (hour_sin, hour_cos)\n",
    "\n",
    "# Get embedding layer parameters\n",
    "num_stations = demand_data['station_id'].nunique()  # Total number of stations\n",
    "station_embedding_dim = 8  # Embedding dimension\n",
    "\n",
    "# Define hidden layer dimension and output dimension\n",
    "hidden_dim = 16\n",
    "output_dim = 1\n",
    "\n",
    "# Initialize the model\n",
    "model = GCN(input_dim, hidden_dim, output_dim, num_stations, station_embedding_dim)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.1238\n",
      "Epoch 20, Loss: 0.0391\n",
      "Epoch 30, Loss: 0.0233\n",
      "Epoch 40, Loss: 0.0157\n",
      "Epoch 50, Loss: 0.0116\n",
      "Epoch 60, Loss: 0.0092\n",
      "Epoch 70, Loss: 0.0080\n",
      "Epoch 80, Loss: 0.0071\n",
      "Epoch 90, Loss: 0.0065\n",
      "Epoch 100, Loss: 0.0060\n",
      "Epoch 110, Loss: 0.0056\n",
      "Epoch 120, Loss: 0.0053\n",
      "Epoch 130, Loss: 0.0050\n",
      "Epoch 140, Loss: 0.0048\n",
      "Epoch 150, Loss: 0.0046\n",
      "Epoch 160, Loss: 0.0044\n",
      "Epoch 170, Loss: 0.0042\n",
      "Epoch 180, Loss: 0.0041\n",
      "Epoch 190, Loss: 0.0040\n",
      "Epoch 200, Loss: 0.0039\n",
      "Epoch 210, Loss: 0.0037\n",
      "Epoch 220, Loss: 0.0036\n",
      "Epoch 230, Loss: 0.0035\n",
      "Epoch 240, Loss: 0.0035\n",
      "Epoch 250, Loss: 0.0034\n",
      "Epoch 260, Loss: 0.0033\n",
      "Epoch 270, Loss: 0.0032\n",
      "Epoch 280, Loss: 0.0032\n",
      "Epoch 290, Loss: 0.0031\n",
      "Epoch 300, Loss: 0.0030\n",
      "Epoch 310, Loss: 0.0030\n",
      "Epoch 320, Loss: 0.0029\n",
      "Epoch 330, Loss: 0.0029\n",
      "Epoch 340, Loss: 0.0028\n",
      "Epoch 350, Loss: 0.0028\n",
      "Epoch 360, Loss: 0.0027\n",
      "Epoch 370, Loss: 0.0027\n",
      "Epoch 380, Loss: 0.0026\n",
      "Epoch 390, Loss: 0.0026\n",
      "Epoch 400, Loss: 0.0025\n",
      "Epoch 410, Loss: 0.0025\n",
      "Epoch 420, Loss: 0.0025\n",
      "Epoch 430, Loss: 0.0024\n",
      "Epoch 440, Loss: 0.0024\n",
      "Epoch 450, Loss: 0.0024\n",
      "Epoch 460, Loss: 0.0024\n",
      "Epoch 470, Loss: 0.0023\n",
      "Epoch 480, Loss: 0.0023\n",
      "Epoch 490, Loss: 0.0023\n",
      "Epoch 500, Loss: 0.0022\n",
      "Epoch 510, Loss: 0.0022\n",
      "Epoch 520, Loss: 0.0022\n",
      "Epoch 530, Loss: 0.0022\n",
      "Epoch 540, Loss: 0.0022\n",
      "Epoch 550, Loss: 0.0021\n",
      "Epoch 560, Loss: 0.0021\n",
      "Epoch 570, Loss: 0.0021\n",
      "Epoch 580, Loss: 0.0021\n",
      "Epoch 590, Loss: 0.0020\n",
      "Epoch 600, Loss: 0.0020\n",
      "Epoch 610, Loss: 0.0020\n",
      "Epoch 620, Loss: 0.0020\n",
      "Epoch 630, Loss: 0.0020\n",
      "Epoch 640, Loss: 0.0020\n",
      "Epoch 650, Loss: 0.0019\n",
      "Epoch 660, Loss: 0.0019\n",
      "Epoch 670, Loss: 0.0019\n",
      "Epoch 680, Loss: 0.0019\n",
      "Epoch 690, Loss: 0.0019\n",
      "Epoch 700, Loss: 0.0019\n",
      "Epoch 710, Loss: 0.0019\n",
      "Epoch 720, Loss: 0.0019\n",
      "Epoch 730, Loss: 0.0018\n",
      "Epoch 740, Loss: 0.0018\n",
      "Epoch 750, Loss: 0.0018\n",
      "Epoch 760, Loss: 0.0018\n",
      "Epoch 770, Loss: 0.0018\n",
      "Epoch 780, Loss: 0.0018\n",
      "Epoch 790, Loss: 0.0018\n",
      "Epoch 800, Loss: 0.0018\n",
      "Epoch 810, Loss: 0.0018\n",
      "Epoch 820, Loss: 0.0018\n",
      "Epoch 830, Loss: 0.0018\n",
      "Epoch 840, Loss: 0.0017\n",
      "Epoch 850, Loss: 0.0017\n",
      "Epoch 860, Loss: 0.0017\n",
      "Epoch 870, Loss: 0.0017\n",
      "Epoch 880, Loss: 0.0017\n",
      "Epoch 890, Loss: 0.0017\n",
      "Epoch 900, Loss: 0.0017\n",
      "Epoch 910, Loss: 0.0017\n",
      "Epoch 920, Loss: 0.0017\n",
      "Epoch 930, Loss: 0.0017\n",
      "Epoch 940, Loss: 0.0017\n",
      "Epoch 950, Loss: 0.0017\n",
      "Epoch 960, Loss: 0.0017\n",
      "Epoch 970, Loss: 0.0016\n",
      "Epoch 980, Loss: 0.0016\n",
      "Epoch 990, Loss: 0.0016\n",
      "Epoch 1000, Loss: 0.0016\n",
      "Epoch 1010, Loss: 0.0016\n",
      "Epoch 1020, Loss: 0.0016\n",
      "Epoch 1030, Loss: 0.0016\n",
      "Epoch 1040, Loss: 0.0016\n",
      "Epoch 1050, Loss: 0.0016\n",
      "Epoch 1060, Loss: 0.0016\n",
      "Epoch 1070, Loss: 0.0016\n",
      "Epoch 1080, Loss: 0.0016\n",
      "Epoch 1090, Loss: 0.0016\n",
      "Epoch 1100, Loss: 0.0016\n",
      "Epoch 1110, Loss: 0.0016\n",
      "Epoch 1120, Loss: 0.0016\n",
      "Epoch 1130, Loss: 0.0016\n",
      "Epoch 1140, Loss: 0.0016\n",
      "Epoch 1150, Loss: 0.0015\n",
      "Epoch 1160, Loss: 0.0015\n",
      "Epoch 1170, Loss: 0.0015\n",
      "Epoch 1180, Loss: 0.0015\n",
      "Epoch 1190, Loss: 0.0015\n",
      "Epoch 1200, Loss: 0.0015\n",
      "Epoch 1210, Loss: 0.0015\n",
      "Epoch 1220, Loss: 0.0015\n",
      "Epoch 1230, Loss: 0.0015\n",
      "Epoch 1240, Loss: 0.0015\n",
      "Epoch 1250, Loss: 0.0015\n",
      "Epoch 1260, Loss: 0.0015\n",
      "Epoch 1270, Loss: 0.0015\n",
      "Epoch 1280, Loss: 0.0015\n",
      "Epoch 1290, Loss: 0.0015\n",
      "Epoch 1300, Loss: 0.0015\n",
      "Epoch 1310, Loss: 0.0015\n",
      "Epoch 1320, Loss: 0.0015\n",
      "Epoch 1330, Loss: 0.0015\n",
      "Epoch 1340, Loss: 0.0015\n",
      "Epoch 1350, Loss: 0.0015\n",
      "Epoch 1360, Loss: 0.0015\n",
      "Epoch 1370, Loss: 0.0015\n",
      "Epoch 1380, Loss: 0.0015\n",
      "Epoch 1390, Loss: 0.0015\n",
      "Epoch 1400, Loss: 0.0015\n",
      "Epoch 1410, Loss: 0.0014\n",
      "Epoch 1420, Loss: 0.0014\n",
      "Epoch 1430, Loss: 0.0014\n",
      "Epoch 1440, Loss: 0.0014\n",
      "Epoch 1450, Loss: 0.0014\n",
      "Epoch 1460, Loss: 0.0014\n",
      "Epoch 1470, Loss: 0.0014\n",
      "Epoch 1480, Loss: 0.0014\n",
      "Epoch 1490, Loss: 0.0014\n",
      "Epoch 1500, Loss: 0.0014\n",
      "Epoch 1510, Loss: 0.0014\n",
      "Epoch 1520, Loss: 0.0014\n",
      "Epoch 1530, Loss: 0.0014\n",
      "Epoch 1540, Loss: 0.0014\n",
      "Epoch 1550, Loss: 0.0014\n",
      "Epoch 1560, Loss: 0.0014\n",
      "Epoch 1570, Loss: 0.0014\n",
      "Epoch 1580, Loss: 0.0014\n",
      "Epoch 1590, Loss: 0.0014\n",
      "Epoch 1600, Loss: 0.0014\n",
      "Epoch 1610, Loss: 0.0014\n",
      "Epoch 1620, Loss: 0.0014\n",
      "Epoch 1630, Loss: 0.0014\n",
      "Epoch 1640, Loss: 0.0014\n",
      "Epoch 1650, Loss: 0.0014\n",
      "Epoch 1660, Loss: 0.0014\n",
      "Epoch 1670, Loss: 0.0014\n",
      "Epoch 1680, Loss: 0.0014\n",
      "Epoch 1690, Loss: 0.0014\n",
      "Epoch 1700, Loss: 0.0014\n",
      "Epoch 1710, Loss: 0.0014\n",
      "Epoch 1720, Loss: 0.0014\n",
      "Epoch 1730, Loss: 0.0014\n",
      "Epoch 1740, Loss: 0.0014\n",
      "Epoch 1750, Loss: 0.0013\n",
      "Epoch 1760, Loss: 0.0013\n",
      "Epoch 1770, Loss: 0.0013\n",
      "Epoch 1780, Loss: 0.0013\n",
      "Epoch 1790, Loss: 0.0013\n",
      "Epoch 1800, Loss: 0.0013\n",
      "Epoch 1810, Loss: 0.0013\n",
      "Epoch 1820, Loss: 0.0013\n",
      "Epoch 1830, Loss: 0.0013\n",
      "Epoch 1840, Loss: 0.0013\n",
      "Epoch 1850, Loss: 0.0013\n",
      "Epoch 1860, Loss: 0.0013\n",
      "Epoch 1870, Loss: 0.0013\n",
      "Epoch 1880, Loss: 0.0013\n",
      "Epoch 1890, Loss: 0.0013\n",
      "Epoch 1900, Loss: 0.0013\n",
      "Epoch 1910, Loss: 0.0013\n",
      "Epoch 1920, Loss: 0.0013\n",
      "Epoch 1930, Loss: 0.0013\n",
      "Epoch 1940, Loss: 0.0013\n",
      "Epoch 1950, Loss: 0.0013\n",
      "Epoch 1960, Loss: 0.0013\n",
      "Epoch 1970, Loss: 0.0013\n",
      "Epoch 1980, Loss: 0.0013\n",
      "Epoch 1990, Loss: 0.0013\n",
      "Epoch 2000, Loss: 0.0013\n",
      "Epoch 2010, Loss: 0.0013\n",
      "Epoch 2020, Loss: 0.0013\n",
      "Epoch 2030, Loss: 0.0013\n",
      "Epoch 2040, Loss: 0.0013\n",
      "Epoch 2050, Loss: 0.0013\n",
      "Epoch 2060, Loss: 0.0013\n",
      "Epoch 2070, Loss: 0.0013\n",
      "Epoch 2080, Loss: 0.0013\n",
      "Epoch 2090, Loss: 0.0013\n",
      "Epoch 2100, Loss: 0.0013\n",
      "Epoch 2110, Loss: 0.0013\n",
      "Epoch 2120, Loss: 0.0013\n",
      "Epoch 2130, Loss: 0.0013\n",
      "Epoch 2140, Loss: 0.0013\n",
      "Epoch 2150, Loss: 0.0013\n",
      "Epoch 2160, Loss: 0.0013\n",
      "Epoch 2170, Loss: 0.0013\n",
      "Epoch 2180, Loss: 0.0013\n",
      "Epoch 2190, Loss: 0.0013\n",
      "Epoch 2200, Loss: 0.0013\n",
      "Epoch 2210, Loss: 0.0013\n",
      "Epoch 2220, Loss: 0.0013\n",
      "Epoch 2230, Loss: 0.0013\n",
      "Epoch 2240, Loss: 0.0013\n",
      "Epoch 2250, Loss: 0.0012\n",
      "Epoch 2260, Loss: 0.0012\n",
      "Epoch 2270, Loss: 0.0012\n",
      "Epoch 2280, Loss: 0.0012\n",
      "Epoch 2290, Loss: 0.0012\n",
      "Epoch 2300, Loss: 0.0012\n",
      "Epoch 2310, Loss: 0.0012\n",
      "Epoch 2320, Loss: 0.0012\n",
      "Epoch 2330, Loss: 0.0012\n",
      "Epoch 2340, Loss: 0.0012\n",
      "Epoch 2350, Loss: 0.0012\n",
      "Epoch 2360, Loss: 0.0012\n",
      "Epoch 2370, Loss: 0.0012\n",
      "Epoch 2380, Loss: 0.0012\n",
      "Epoch 2390, Loss: 0.0012\n",
      "Epoch 2400, Loss: 0.0012\n",
      "Epoch 2410, Loss: 0.0012\n",
      "Epoch 2420, Loss: 0.0012\n",
      "Epoch 2430, Loss: 0.0012\n",
      "Epoch 2440, Loss: 0.0012\n",
      "Epoch 2450, Loss: 0.0012\n",
      "Epoch 2460, Loss: 0.0012\n",
      "Epoch 2470, Loss: 0.0012\n",
      "Epoch 2480, Loss: 0.0012\n",
      "Epoch 2490, Loss: 0.0012\n",
      "Epoch 2500, Loss: 0.0012\n",
      "Epoch 2510, Loss: 0.0012\n",
      "Epoch 2520, Loss: 0.0012\n",
      "Epoch 2530, Loss: 0.0012\n",
      "Epoch 2540, Loss: 0.0012\n",
      "Epoch 2550, Loss: 0.0012\n",
      "Epoch 2560, Loss: 0.0012\n",
      "Epoch 2570, Loss: 0.0012\n",
      "Epoch 2580, Loss: 0.0012\n",
      "Epoch 2590, Loss: 0.0012\n",
      "Epoch 2600, Loss: 0.0012\n",
      "Epoch 2610, Loss: 0.0012\n",
      "Epoch 2620, Loss: 0.0012\n",
      "Epoch 2630, Loss: 0.0012\n",
      "Epoch 2640, Loss: 0.0012\n",
      "Epoch 2650, Loss: 0.0012\n",
      "Epoch 2660, Loss: 0.0012\n",
      "Epoch 2670, Loss: 0.0012\n",
      "Epoch 2680, Loss: 0.0012\n",
      "Epoch 2690, Loss: 0.0012\n",
      "Epoch 2700, Loss: 0.0012\n",
      "Epoch 2710, Loss: 0.0012\n",
      "Epoch 2720, Loss: 0.0012\n",
      "Epoch 2730, Loss: 0.0012\n",
      "Epoch 2740, Loss: 0.0012\n",
      "Epoch 2750, Loss: 0.0012\n",
      "Epoch 2760, Loss: 0.0012\n",
      "Epoch 2770, Loss: 0.0012\n",
      "Epoch 2780, Loss: 0.0012\n",
      "Epoch 2790, Loss: 0.0012\n",
      "Epoch 2800, Loss: 0.0012\n",
      "Epoch 2810, Loss: 0.0012\n",
      "Epoch 2820, Loss: 0.0012\n",
      "Epoch 2830, Loss: 0.0012\n",
      "Epoch 2840, Loss: 0.0012\n",
      "Epoch 2850, Loss: 0.0012\n",
      "Epoch 2860, Loss: 0.0012\n",
      "Epoch 2870, Loss: 0.0012\n",
      "Epoch 2880, Loss: 0.0012\n",
      "Epoch 2890, Loss: 0.0012\n",
      "Epoch 2900, Loss: 0.0012\n",
      "Epoch 2910, Loss: 0.0012\n",
      "Epoch 2920, Loss: 0.0012\n",
      "Epoch 2930, Loss: 0.0012\n",
      "Epoch 2940, Loss: 0.0012\n",
      "Epoch 2950, Loss: 0.0012\n",
      "Epoch 2960, Loss: 0.0012\n",
      "Epoch 2970, Loss: 0.0012\n",
      "Epoch 2980, Loss: 0.0012\n",
      "Epoch 2990, Loss: 0.0012\n",
      "Epoch 3000, Loss: 0.0012\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Step 6: Train the model\n",
    "# ----------------------------\n",
    "\n",
    "# Define the model\n",
    "input_dim = 2  # Time feature dimensions (hour_sin, hour_cos)\n",
    "hidden_dim = 16\n",
    "output_dim = 1\n",
    "model = GCN(input_dim, hidden_dim, output_dim, num_stations, station_embedding_dim)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Start training\n",
    "for epoch in range(3000):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    # Pass time_features and station_ids to the model\n",
    "    out = model(time_features, station_ids, edge_index)\n",
    "    loss = criterion(out[train_mask], y[train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch {epoch + 1}, Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0023\n",
      "Predicted: 4.62, Actual: 10.00\n",
      "Predicted: 2.21, Actual: 2.00\n",
      "Predicted: 13.89, Actual: 12.00\n",
      "Predicted: 1.00, Actual: 1.00\n",
      "Predicted: 3.43, Actual: 1.00\n",
      "Predicted: 48.89, Actual: 41.00\n",
      "Predicted: 14.49, Actual: 22.00\n",
      "Predicted: 18.50, Actual: 16.00\n",
      "Predicted: 1.00, Actual: 1.00\n",
      "Predicted: 4.81, Actual: 1.00\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Step 7: Evaluate the model\n",
    "# ----------------------------\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # 传入 time_features 和 station_ids\n",
    "    predictions = model(time_features, station_ids, edge_index)\n",
    "    test_loss = criterion(predictions[test_mask], y[test_mask])\n",
    "    print(f'Test Loss: {test_loss.item():.4f}')\n",
    "\n",
    "# Denormalize predictions for comparison\n",
    "total_demand_scaled = np.zeros((predictions.shape[0], 1))  # Only total_demand\n",
    "total_demand_scaled[:, 0] = predictions.detach().numpy().flatten()\n",
    "predictions_denormalized = scaler.inverse_transform(total_demand_scaled)[:, 0]\n",
    "\n",
    "actual_scaled = np.zeros((y.shape[0], 1))\n",
    "actual_scaled[:, 0] = y.numpy().flatten()\n",
    "actual_denormalized = scaler.inverse_transform(actual_scaled)[:, 0]\n",
    "\n",
    "# Print sample predictions vs actual values\n",
    "for i in range(10):\n",
    "    print(f'Predicted: {max(1, predictions_denormalized[test_mask[i]]):.2f}, Actual: {actual_denormalized[test_mask[i]]:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 stations with the highest predicted demand:\n",
      "               station_name  hour\n",
      "350   Ashland Ave & 50th St    14\n",
      "2871      Clark St & Elm St    15\n",
      "2872      Clark St & Elm St    16\n",
      "2870      Clark St & Elm St    14\n",
      "4479  Dearborn St & Erie St    15\n",
      "2873      Clark St & Elm St    17\n",
      "4480  Dearborn St & Erie St    16\n",
      "2869      Clark St & Elm St    13\n",
      "4478  Dearborn St & Erie St    14\n",
      "1071   Broadway & Barry Ave    15\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Step 8: Predict future demand\n",
    "# ----------------------------\n",
    "\n",
    "# Generate future time features for the next 24 hours\n",
    "future_hours = np.arange(24)  # Hours from 0 to 23\n",
    "future_time_features = pd.DataFrame({\n",
    "    'hour': np.tile(future_hours, num_stations),\n",
    "    'hour_sin': np.tile(np.sin(2 * np.pi * future_hours / 24), num_stations),\n",
    "    'hour_cos': np.tile(np.cos(2 * np.pi * future_hours / 24), num_stations),\n",
    "    'station_id': np.repeat(np.arange(num_stations), 24)\n",
    "})\n",
    "\n",
    "# Ensure station_id is mapped correctly and consistently\n",
    "future_time_features['station_name'] = future_time_features['station_id'].map(\n",
    "    {v: k for k, v in station_index_map.items()}\n",
    ")\n",
    "\n",
    "# Drop duplicate entries (if any, just in case)\n",
    "future_time_features = future_time_features.drop_duplicates(subset=['station_name', 'hour'])\n",
    "\n",
    "# Convert to tensors for prediction\n",
    "future_station_ids = torch.tensor(future_time_features['station_id'].values, dtype=torch.long)\n",
    "future_time_tensor = torch.tensor(future_time_features[['hour_sin', 'hour_cos']].values, dtype=torch.float)\n",
    "\n",
    "# Predict demand for each station and hour\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    future_predictions = model(future_time_tensor, future_station_ids, edge_index)\n",
    "\n",
    "# Denormalize predictions\n",
    "future_predictions_scaled = future_predictions.numpy().flatten()\n",
    "future_predictions_denormalized = scaler.inverse_transform(future_predictions_scaled.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Add predictions back to DataFrame\n",
    "future_time_features['predicted_demand'] = future_predictions_denormalized\n",
    "\n",
    "# ----------------------------\n",
    "# Step 9: Find top 10 stations and times\n",
    "# ----------------------------\n",
    "\n",
    "# Sort by predicted demand in descending order\n",
    "top_10 = future_time_features.sort_values(by='predicted_demand', ascending=False).head(10)\n",
    "\n",
    "# Ensure station_name and hour are unique in the results\n",
    "top_10 = top_10.drop_duplicates(subset=['station_name', 'hour'])\n",
    "\n",
    "# Map station_id back to station names (already done)\n",
    "id_to_station = {v: k for k, v in station_index_map.items()}\n",
    "top_10['station_name'] = top_10['station_id'].map(id_to_station)\n",
    "\n",
    "# Print the top 10 results\n",
    "print(\"Top 10 stations with the highest predicted demand:\")\n",
    "print(top_10[['station_name', 'hour']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
