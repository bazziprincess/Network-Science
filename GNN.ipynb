{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import \n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from geopy.distance import geodesic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Step 1: Load and preprocess data\n",
    "# ----------------------------\n",
    "# Load the bike trip data\n",
    "data = pd.read_csv('zhong.csv')\n",
    "\n",
    "# Convert time columns to datetime format and extract features\n",
    "data['start_time'] = pd.to_datetime(data['start_time'])\n",
    "data['hour'] = data['start_time'].dt.hour\n",
    "\n",
    "# Count hourly inflows and outflows\n",
    "daily_inflow = data.groupby(['end_station_name', 'hour']).size().rename('inflow')\n",
    "daily_outflow = data.groupby(['start_station_name', 'hour']).size().rename('outflow')\n",
    "\n",
    "# Combine inflow and outflow into a single dataset\n",
    "demand_data = pd.concat([daily_inflow, daily_outflow], axis=1).fillna(0)\n",
    "demand_data['total_demand'] = demand_data['inflow'] + demand_data['outflow']\n",
    "\n",
    "# Normalize demand data\n",
    "scaler = MinMaxScaler()\n",
    "demand_data[['inflow', 'outflow', 'total_demand']] = scaler.fit_transform(demand_data[['inflow', 'outflow', 'total_demand']])\n",
    "\n",
    "# Reset index for easier handling\n",
    "demand_data = demand_data.reset_index()\n",
    "\n",
    "# Rename 'level_0' to 'station_name' for clarity\n",
    "demand_data = demand_data.rename(columns={'level_0': 'station_name'})\n",
    "\n",
    "# Create station mapping for node index\n",
    "demand_data['station_id'] = demand_data['station_name'].factorize()[0]\n",
    "\n",
    "# ----------------------------\n",
    "# Step 2: Create graph structure\n",
    "# ----------------------------\n",
    "# Load station coordinates for edges\n",
    "station_coords = data[['start_station_name', 'start_lat', 'start_lng']].drop_duplicates()\n",
    "station_coords.columns = ['station_name', 'lat', 'lng']\n",
    "\n",
    "# Build edge connections based on station proximity\n",
    "edges = []\n",
    "edge_weights = []\n",
    "stations = station_coords.to_dict('records')\n",
    "\n",
    "for i, station1 in enumerate(stations):\n",
    "    for j, station2 in enumerate(stations):\n",
    "        if i != j:\n",
    "            distance = geodesic((station1['lat'], station1['lng']), (station2['lat'], station2['lng'])).kilometers\n",
    "            if distance <= 1.0:  # Only consider stations within 1 km\n",
    "                edges.append((station1['station_name'], station2['station_name']))\n",
    "                edge_weights.append(1 / distance)  # Weight inversely proportional to distance\n",
    "\n",
    "# Convert station names to indices for PyTorch Geometric\n",
    "station_index_map = dict(zip(demand_data['station_name'], demand_data['station_id']))\n",
    "edges = [(station_index_map[edge[0]], station_index_map[edge[1]]) for edge in edges]\n",
    "edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "edge_attr = torch.tensor(edge_weights, dtype=torch.float)\n",
    "\n",
    "# ----------------------------\n",
    "# Step 3: Prepare node features and labels\n",
    "# ----------------------------\n",
    "# Node features: inflow, outflow, and hour\n",
    "node_features = demand_data[['inflow', 'outflow', 'hour']].values\n",
    "x = torch.tensor(node_features, dtype=torch.float)\n",
    "\n",
    "# Labels: total demand\n",
    "labels = demand_data['total_demand'].values\n",
    "y = torch.tensor(labels, dtype=torch.float).unsqueeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Step 4: Split data into train/test\n",
    "# ----------------------------\n",
    "train_mask, test_mask = train_test_split(range(x.size(0)), test_size=0.2, random_state=42)\n",
    "train_mask = torch.tensor(train_mask, dtype=torch.long)\n",
    "test_mask = torch.tensor(test_mask, dtype=torch.long)\n",
    "\n",
    "# ----------------------------\n",
    "# Step 5: Define GNN model\n",
    "# ----------------------------\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "input_dim = x.size(1)\n",
    "hidden_dim = 16\n",
    "output_dim = 1\n",
    "model = GCN(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.1013\n",
      "Epoch 20, Loss: 0.0706\n",
      "Epoch 30, Loss: 0.0184\n",
      "Epoch 40, Loss: 0.0185\n",
      "Epoch 50, Loss: 0.0118\n",
      "Epoch 60, Loss: 0.0087\n",
      "Epoch 70, Loss: 0.0062\n",
      "Epoch 80, Loss: 0.0047\n",
      "Epoch 90, Loss: 0.0035\n",
      "Epoch 100, Loss: 0.0026\n",
      "Epoch 110, Loss: 0.0020\n",
      "Epoch 120, Loss: 0.0015\n",
      "Epoch 130, Loss: 0.0012\n",
      "Epoch 140, Loss: 0.0010\n",
      "Epoch 150, Loss: 0.0009\n",
      "Epoch 160, Loss: 0.0008\n",
      "Epoch 170, Loss: 0.0008\n",
      "Epoch 180, Loss: 0.0008\n",
      "Epoch 190, Loss: 0.0007\n",
      "Epoch 200, Loss: 0.0007\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Step 6: Train the model\n",
    "# ----------------------------\n",
    "for epoch in range(200):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(x, edge_index)\n",
    "    loss = criterion(out[train_mask], y[train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch {epoch + 1}, Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0007\n",
      "Predicted: 2.16, Actual: 2.00\n",
      "Predicted: 20.78, Actual: 22.00\n",
      "Predicted: 2.52, Actual: 2.00\n",
      "Predicted: 16.51, Actual: 1.00\n",
      "Predicted: 39.50, Actual: 38.00\n",
      "Predicted: 74.41, Actual: 74.00\n",
      "Predicted: 5.29, Actual: 6.00\n",
      "Predicted: 33.89, Actual: 31.00\n",
      "Predicted: 46.28, Actual: 43.00\n",
      "Predicted: 8.93, Actual: 8.00\n",
      "Predicted: 88.54, Actual: 87.00\n",
      "Predicted: 3.33, Actual: 3.00\n",
      "Predicted: 14.29, Actual: 15.00\n",
      "Predicted: 16.93, Actual: 17.00\n",
      "Predicted: 17.54, Actual: 18.00\n",
      "Predicted: 2.13, Actual: 2.00\n",
      "Predicted: 55.65, Actual: 56.00\n",
      "Predicted: 46.27, Actual: 46.00\n",
      "Predicted: 5.97, Actual: 5.00\n",
      "Predicted: 46.13, Actual: 45.00\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Step 7: Evaluate the model\n",
    "# ----------------------------\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(x, edge_index)\n",
    "    test_loss = criterion(predictions[test_mask], y[test_mask])\n",
    "    print(f'Test Loss: {test_loss.item():.4f}')\n",
    "\n",
    "total_demand_scaled = np.zeros((predictions.shape[0], 3))  # Create a placeholder array with 3 columns\n",
    "total_demand_scaled[:, 2] = predictions.detach().numpy().flatten()  # Only set the total_demand column\n",
    "predictions_denormalized = scaler.inverse_transform(total_demand_scaled)[:, 2]\n",
    "\n",
    "actual_scaled = np.zeros((y.shape[0], 3))\n",
    "actual_scaled[:, 2] = y.numpy().flatten()\n",
    "actual_denormalized = scaler.inverse_transform(actual_scaled)[:, 2]\n",
    "\n",
    "# Print sample predictions vs actual values\n",
    "for i in range(20):\n",
    "    print(f'Predicted: {predictions_denormalized[test_mask[i]]:.2f}, Actual: {actual_denormalized[test_mask[i]]:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
